from dotenv import load_dotenv
from openai import OpenAI
import os
import langchain
from functions import load_documents , text_splitter , docs2str , vector_store_as_retriever
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts import MessagesPlaceholder
from langchain_classic.chains.history_aware_retriever import create_history_aware_retriever
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_classic.chains.retrieval import create_retrieval_chain
from langchain_community.tools.tavily_search import TavilySearchResults

load_dotenv(override = True)

langchain_api_key = os.getenv("LANGCHAIN_API_KEY")
os.environ['LANGCHAIN_PROJECT'] = "LLM Architecture Explainer"
openai_api_key = os.getenv("OPENAI_API_KEY")
tavily_api_key = os.getenv("TAVILY_API_KEY")


from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

search = TavilySearchResults(k=3)

llm_with_search = llm.bind_tools([search])

output_parser = StrOutputParser()



def Chroma_database_creation(folder_path : str):
    documents = load_documents(folder_path)
    splits = text_splitter(documents)
    retriever = vector_store_as_retriever(splits)
    return retriever





def rag_chain(retriever):
    contextualize_q_system_prompt = """
    Given a chat history and the latest user question
    which might reference context in the chat history,
    formulate a standalone question which can be understood
    without the chat history. Do NOT answer the question,
    just reformulate it if needed and otherwise return it as is.
    """

    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    history_aware_retriever = create_history_aware_retriever(
        llm_with_search, retriever, contextualize_q_prompt
    )



    qa_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a AI assistant trained to be an LLM Architecture Explainer. Use the following context to"
    " answer the user's question as well as your understanding of LLMs.Use the context provided to you as a tool"
    "rather than the gospel to answer the question, however do reap the benefit of the given tool to the fullest"
    "logical extent without giving the best possible answer that you could.If in doubt,do use the websearch tool"
    "provided to you to clarify/ improve the answer."),
    ("system", "Context: {context}"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
    ])

    review_prompt = ChatPromptTemplate.from_messages([
         (
        "system",
        """
    You are a reviewing AI agent.

    You will be given:
    - the user's latest question
    - an initial answer generated by a RAG system
    - retrieved document context
    - the chat history so far

    Your task is to decide whether the initial answer is acceptable.

    Acceptance criteria:
    1. The answer must be relevant to the latest question.
    2. The answer must be supported by the retrieved context.

    Instructions:
    - If the answer satisfies BOTH criteria, return it exactly as-is.
    - If the answer fails either criterion, generate a new answer using:
    - the retrieved context as the primary source
    - the chat history only for disambiguation
    - web search ONLY if the retrieved context is insufficient

    Rules:
    - Do NOT hallucinate facts.
    - If neither the retrieved context nor web search contains enough information, say:
    "I don't have enough information in the provided documents to answer this question."
    - Be concise, technical, and accurate.
    """
    ),
    ("system", "Context: {context}"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")    
    ])

    review_chain = review_prompt | llm_with_search | output_parser


    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    rag_chain = rag_chain | review_chain
    return rag_chain